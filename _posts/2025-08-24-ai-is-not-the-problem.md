---
title: "AI is not the problem"
date: 2025-08-24
---

I’ve seen the claim that *AI is a bubble* repeated over and over in articles and online discussions.

I don’t think that’s true.

First of all, most people talk about “AI” while really meaning **LLMs** and their applications. Many AI researchers would already raise an eyebrow at this conflation.

But let’s set that aside and focus on LLMs, taking a step back.

Since the first release of ChatGPT, Transformer-based technology (a specific neural network architecture LLMs are base on) has been labeled revolutionary—some even suggesting it could bring us to AGI (Artificial General Intelligence) or even ASI (Artificial Super Intelligence).  

The massive investments and the hype-filled narrative around every new feature or benchmark have created a huge buzz. At the same time, that buzz naturally triggered skepticism. That’s not new. Every disruptive innovation brings both hype and doubt.

The biggest fear? **Jobs disappearing.**  
But let’s be honest, that’s how it’s always been. Every technological revolution reshapes work. When was the last time you saw a knife sharpener on the street?

I believe LLMs are truly revolutionary technology with impact on all of us, one way or another. Closing our eyes or rejecting them out of fear isn’t the solution. The real risk is building up a **technological gap** that becomes harder to close over time.

And the first to adopt them should be software developers.  

I often hear devs say they don’t want to use LLMs because they fear “losing cognitive skills.” In my view, using LLMs to help write code is a huge advantage, but thinking that code can be used “as is,” without human review, understanding, and acceptance, is a mistake. Without a human in the loop, you’ll end up with sloppy, verbose, unmaintainable code. That’s not an *if*, it’s a certainty.

Personally, I use **Cursor** to code, and it’s a powerful tool. But to get meaningful results, you need to invest effort: prompt engineering, system architecture, breaking down problems into the smallest possible subproblems. Otherwise, any LLM struggles. And the more you iterate blindly with an LLM, the more bloated your code becomes. If you’ve tried it, you know exactly what I mean.

That’s why I agree with Bezos: it’s dumb to think about replacing software developers with AI. AI is an **accelerator**. It may reduce the size of teams (since one dev with AI can do more), but it will also enable more startups and smaller teams to bring ambitious ideas to life.  

That means more diverse opportunities, not fewer.

So the real issue isn’t that LLMs are a bubble, or that they’ll “take all the jobs.”  
The issue is that we need to **learn how to use them.** Developers should leverage them for code review, reducing repetitive boilerplate work, and crucially—understand them deeply enough to integrate them into the systems they’re building.

LLMs are here to stay. They might eventually be replaced by other architectures or entirely new technologies. Who knows? The future is unpredictable. But one thing is clear: **we’re not going back.**  

So the best thing we can do is acknowledge it, prepare, study, and practice—again and again. Don’t demonize them, don’t mystify them. Treat them as what they are: tools. And like any powerful tool, the value depends on how well we learn to use them.


Have a great week and take care,

Federico  

---

## Takeaways

- LLMs are not a bubble. The stock market might think so, but markets don’t care about technology—they care about speculation.  
- Rejecting LLMs (and other AI technologies) is a mistake if you’re a software developer (and not only).  
- Jobs evolve, disappear, and transform. That’s always been the case.
